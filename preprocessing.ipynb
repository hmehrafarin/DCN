{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-MPzuXs7Lkx",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "47839e56-d5b8-47be-f0aa-f593aa1722f4"
      },
      "source": [
        "#@title Download nltk data\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMVb-1byuFLE",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Necessary Imports\n",
        "import argparse\n",
        "import linecache\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import imp\n",
        "import os\n",
        "import sys\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import gzip\n",
        "import re\n",
        "import tarfile\n",
        "import argparse\n",
        "from os.path import join as pjoin\n",
        "from six.moves import urllib\n",
        "import tensorflow.compat.v1.gfile as gfile\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GloLS-6vkqs",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtFpGanztjWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reporthook(t):\n",
        "  \"\"\"https://github.com/tqdm/tqdm\"\"\"\n",
        "  last_b = [0]\n",
        "\n",
        "  def inner(b=1, bsize=1, tsize=None):\n",
        "    # for showing progress \n",
        "    if tsize is not None:\n",
        "        t.total = tsize\n",
        "    t.update((b - last_b[0]) * bsize)\n",
        "    last_b[0] = b\n",
        "  return inner\n",
        "\n",
        "def maybe_download(url, filename, prefix, num_bytes=None):\n",
        "# Takes an URL download the contents and returns the filename\n",
        "\n",
        "  local_filename = None\n",
        "  if not os.path.exists(os.path.join(prefix, filename)):  \n",
        "\n",
        "    print(\"Downloading file {}...\".format(url + filename))\n",
        "    with tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
        "      local_filename, _ = urlretrieve(url + filename, os.path.join(prefix,filename), reporthook=reporthook(t))\n",
        "        \n",
        "  return local_filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrfDR5Ef4TOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_from_json(filename):\n",
        "  # Reading data in json format\n",
        "  with open(filename) as data_file:\n",
        "    data = json.load(data_file)\n",
        "  return data\n",
        "\n",
        "def tokenize(sequence):\n",
        "  tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"') for token in nltk.word_tokenize(sequence)]\n",
        "  return [x for x in tokens] \n",
        "\n",
        "def token_idx_map(context, context_tokens):\n",
        "  acc = ''\n",
        "  current_token_idx = 0\n",
        "  token_map = dict()\n",
        "  for char_idx, char in enumerate(context):\n",
        "    if char != ' ':\n",
        "      acc += char\n",
        "      context_token = str(context_tokens[current_token_idx])\n",
        "      if acc == context_token:\n",
        "        syn_start = char_idx - len(acc) + 1\n",
        "        token_map[syn_start] = [acc, current_token_idx]\n",
        "        acc = ''\n",
        "        current_token_idx += 1\n",
        "  return token_map\n",
        "\n",
        "def read_write_dataset(dataset, tier, prefix):\n",
        "# Reads the dataset extracts context, question, answer\n",
        "\n",
        "    qn, an = 0, 0\n",
        "    skipped = 0\n",
        "\n",
        "    with open(os.path.join(prefix, tier +'.context'), 'w') as context_file,  \\\n",
        "         open(os.path.join(prefix, tier +'.question'), 'w') as question_file,\\\n",
        "         open(os.path.join(prefix, tier +'.answer'), 'w') as text_file, \\\n",
        "         open(os.path.join(prefix, tier +'.span'), 'w') as span_file:\n",
        "\n",
        "        for articles_id in tqdm(list(range(len(dataset['data']))), desc=\"Preprocessing {}\".format(tier)):\n",
        "            article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
        "            for pid in range(len(article_paragraphs)):\n",
        "                context = article_paragraphs[pid]['context']\n",
        "               \n",
        "                context = context.replace(\"''\", '\" ')\n",
        "                context = context.replace(\"``\", '\" ')\n",
        "\n",
        "                context_tokens = tokenize(context)\n",
        "                answer_map = token_idx_map(context, context_tokens)\n",
        "\n",
        "                qas = article_paragraphs[pid]['qas']\n",
        "                for qid in range(len(qas)):\n",
        "                    question = qas[qid]['question']\n",
        "                    question_tokens = tokenize(question)\n",
        "\n",
        "                    answers = qas[qid]['answers']\n",
        "                    qn += 1\n",
        "\n",
        "                    num_answers = list(range(1))\n",
        "\n",
        "                    for ans_id in num_answers:\n",
        "                        # it contains answer_start, text\n",
        "                        text = qas[qid]['answers'][ans_id]['text']\n",
        "                        a_s = qas[qid]['answers'][ans_id]['answer_start']\n",
        "\n",
        "                        text_tokens = tokenize(text)\n",
        "\n",
        "                        answer_start = qas[qid]['answers'][ans_id]['answer_start']\n",
        "\n",
        "                        answer_end = answer_start + len(text)\n",
        "\n",
        "                        last_word_answer = len(text_tokens[-1]) # add one to get the first char\n",
        "\n",
        "                        try:\n",
        "                            a_start_idx = answer_map[answer_start][1]\n",
        "\n",
        "                            a_end_idx = answer_map[answer_end - last_word_answer][1]\n",
        "\n",
        "                            # remove length restraint since we deal with it later\n",
        "                            context_file.write(' '.join(context_tokens) + '\\n')\n",
        "                            question_file.write(' '.join(question_tokens) + '\\n')\n",
        "                            text_file.write(' '.join(text_tokens) + '\\n')\n",
        "                            span_file.write(' '.join([str(a_start_idx), str(a_end_idx)]) + '\\n')\n",
        "\n",
        "                        except Exception as e:\n",
        "                            skipped += 1\n",
        "\n",
        "                        an += 1\n",
        "\n",
        "    print(\"Skipped {} question/answer pairs in {}\".format(skipped, tier))\n",
        "    return qn,an\n",
        "\n",
        "def save_files(prefix, tier, indices):\n",
        "  with open(os.path.join(prefix, tier + '.context'), 'w') as context_file,  \\\n",
        "     open(os.path.join(prefix, tier + '.question'), 'w') as question_file,\\\n",
        "     open(os.path.join(prefix, tier + '.answer'), 'w') as text_file, \\\n",
        "     open(os.path.join(prefix, tier + '.span'), 'w') as span_file:\n",
        "\n",
        "    for i in indices:\n",
        "      context_file.write(linecache.getline(os.path.join(prefix, 'train.context'), i))\n",
        "      question_file.write(linecache.getline(os.path.join(prefix, 'train.question'), i))\n",
        "      text_file.write(linecache.getline(os.path.join(prefix, 'train.answer'), i))\n",
        "      span_file.write(linecache.getline(os.path.join(prefix, 'train.span'), i))\n",
        "\n",
        "\n",
        "def split_tier(prefix, train_percentage = 0.9, shuffle=False):\n",
        "  # Get number of lines in file\n",
        "  context_filename = os.path.join(prefix, 'train' + '.context')\n",
        "  # Get the number of lines\n",
        "  with open(context_filename) as current_file:\n",
        "    num_lines = sum(1 for line in current_file)\n",
        "  # Get indices and split into two files\n",
        "  indices_dev = list(range(num_lines))[int(num_lines * train_percentage)::]\n",
        "  if shuffle:\n",
        "    np.random.shuffle(indices_dev)\n",
        "    print(\"Shuffling...\")\n",
        "  save_files(prefix, 'val', indices_dev)\n",
        "  indices_train = list(range(num_lines))[:int(num_lines * train_percentage)]\n",
        "  if shuffle:\n",
        "    np.random.shuffle(indices_train)\n",
        "  save_files(prefix, 'train', indices_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csTBX3YLp3WY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "fa7741c3-9e0b-4012-8ea0-aa98dfef7fd3"
      },
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "squad_base_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  download_prefix = os.path.join(\"download\", \"squad\")\n",
        "  data_prefix = os.path.join(\"data\", \"squad\")\n",
        "\n",
        "  print(\"Downloading datasets into {}\".format(download_prefix))\n",
        "  print(\"Preprocessing datasets into {}\".format(data_prefix))\n",
        "\n",
        "  if not os.path.exists(download_prefix):\n",
        "    os.makedirs(download_prefix)\n",
        "  if not os.path.exists(data_prefix):\n",
        "    os.makedirs(data_prefix)\n",
        "\n",
        "  train_filename = \"train-v1.1.json\"\n",
        "  dev_filename = \"dev-v1.1.json\"\n",
        "\n",
        "  # Downloading dataset\n",
        "  maybe_download(squad_base_url, train_filename, download_prefix, 30288272)\n",
        "\n",
        "  train_data = data_from_json(os.path.join(download_prefix, train_filename))\n",
        "\n",
        "  train_num_questions, train_num_answers = read_write_dataset(train_data, 'train', data_prefix)\n",
        "\n",
        "  # 1. Split train into train and validation into 90-10\n",
        "  # 2. Shuffle train, validation\n",
        "  print(\"Splitting the dataset into train and validation\")\n",
        "  split_tier(data_prefix, shuffle=True)\n",
        "\n",
        "  print(\"Processed {} questions and {} answers in train\".format(train_num_questions, train_num_answers))\n",
        "\n",
        "  print(\"Downloading {}\".format(dev_filename))\n",
        "  dev_dataset = maybe_download(squad_base_url, dev_filename, download_prefix, 4854279)\n",
        "\n",
        "  # In dev, we have 10k+ questions, and around 3 answers per question (totaling\n",
        "  # around 34k+ answers).\n",
        "  dev_data = data_from_json(os.path.join(download_prefix, dev_filename))\n",
        "  # list_topics(dev_data)\n",
        "  dev_num_questions, dev_num_answers = read_write_dataset(dev_data, 'dev', data_prefix)\n",
        "  print(\"Processed {} questions and {} answers in dev\".format(dev_num_questions, dev_num_answers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rtrain-v1.1.json: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets into download/squad\n",
            "Preprocessing datasets into data/squad\n",
            "Downloading file https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train-v1.1.json: 30.3MB [00:01, 28.7MB/s]                           \n",
            "Preprocessing train: 100%|██████████| 442/442 [00:48<00:00,  9.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipped 763 question/answer pairs in train\n",
            "Splitting the dataset into train and validation\n",
            "Shuffling...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rdev-v1.1.json: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processed 87599 questions and 87599 answers in train\n",
            "Downloading dev-v1.1.json\n",
            "Downloading file https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "dev-v1.1.json: 4.86MB [00:00, 9.18MB/s]                            \n",
            "Preprocessing dev: 100%|██████████| 48/48 [00:05<00:00,  8.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipped 177 question/answer pairs in dev\n",
            "Processed 10570 questions and 10570 answers in dev\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vf0E62P6POn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/data /content/drive/My\\ Drive/Deep\\ Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhvC-B5PDTEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/download /content/drive/My\\ Drive/Deep\\ Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8MdIwdh-G1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10d03a72-28ef-4ddd-a46a-2004589493b0"
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  glove_base_url = \"http://nlp.stanford.edu/data/\"\n",
        "  glove_filename = \"glove.6B.zip\"\n",
        "    \n",
        "  prefix = os.path.join(\"download\", \"dwr\")\n",
        "\n",
        "  print(\"Storing datasets in {}\".format(prefix))\n",
        "\n",
        "  if not os.path.exists(prefix):\n",
        "    os.makedirs(prefix)\n",
        "    glove_zip = maybe_download(glove_base_url, glove_filename, prefix, 862182613)\n",
        "\n",
        "  glove_zip_ref = zipfile.ZipFile(os.path.join(prefix, glove_filename), 'r')\n",
        "\n",
        "  glove_zip_ref.extractall(prefix)\n",
        "  glove_zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Storing datasets in download/dwr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIml_VZpDeSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/download/dwr /content/drive/My\\ Drive/Deep\\ Data/download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJZJUrnySbw8",
        "colab_type": "text"
      },
      "source": [
        "# Creating id file using ntlk tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXGBsXMUSiJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "cb411e7b-aae4-4698-c219-993ee1b90eb0"
      },
      "source": [
        "\n",
        "_PAD = \"<pad>\"\n",
        "_SOS = \"<sos>\"\n",
        "_UNK = \"<unk>\"\n",
        "_START_VOCAB = [_PAD, _SOS, _UNK]\n",
        "\n",
        "PAD_ID = 0\n",
        "SOS_ID = 1\n",
        "UNK_ID = 2\n",
        "\n",
        "def tokenize(sequence):\n",
        "  tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"') for token in nltk.word_tokenize(sequence)]\n",
        "  return [x for x in tokens] \n",
        "\n",
        "def invert_map(answer_map):\n",
        "  return {v[1]: [v[0], k] for k, v in answer_map.items()}\n",
        "\n",
        "def basic_tokenizer(sentence):\n",
        "  words = []\n",
        "  for space_separated_fragment in sentence.strip().split():\n",
        "    words.extend(re.split(\" \", space_separated_fragment.decode('utf-8')))\n",
        "  return [w for w in words if w]\n",
        "\n",
        "def initialize_vocabulary(vocabulary_path):\n",
        "  \n",
        "  # create vocab file\n",
        "  rev_vocab = []\n",
        "  with gfile.GFile(vocabulary_path, mode=\"r\") as f:\n",
        "    rev_vocab.extend(f.readlines())\n",
        "  rev_vocab = [line.strip('\\n') for line in rev_vocab]\n",
        "  vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "  return vocab, rev_vocab\n",
        "    \n",
        "\n",
        "def process_glove(glove_dir, glove_dim, vocab_list, save_path, size=4e5, random_init=True):\n",
        "\n",
        "    if not gfile.Exists(save_path + \".npz\"):\n",
        "       \n",
        "        glove_path = os.path.join(glove_dir, \"glove.6B.{}d.txt\".format(glove_dim))\n",
        "\n",
        "        if random_init:\n",
        "            glove = np.random.randn(len(vocab_list), glove_dim)\n",
        "        else:\n",
        "            glove = np.zeros((len(vocab_list), glove_dim))\n",
        "        found = 0\n",
        "        with open(glove_path, 'r', encoding='utf8') as fh:  \n",
        "            for line in tqdm(fh, total=size):\n",
        "                array = line.lstrip().rstrip().split(\" \")\n",
        "                word = array[0]\n",
        "                vector = list(map(float, array[1:]))\n",
        "                if word in vocab_list:\n",
        "                  idx = vocab_list.index(word)\n",
        "                  glove[idx, :] = vector\n",
        "                  found += 1\n",
        "                elif word.capitalize() in vocab_list:\n",
        "                  idx = vocab_list.index(word.capitalize())\n",
        "                  glove[idx, :] = vector\n",
        "                  found += 1\n",
        "                elif word.lower() in vocab_list:\n",
        "                  idx = vocab_list.index(word.lower())\n",
        "                  glove[idx, :] = vector\n",
        "                  found += 1\n",
        "                elif word.upper() in vocab_list:\n",
        "                  idx = vocab_list.index(word.upper())\n",
        "                  glove[idx, :] = vector\n",
        "                  found += 1\n",
        "\n",
        "        print(\"{}/{} of word vocab have corresponding vectors in {}\".format(found, len(vocab_list), glove_path))\n",
        "        np.savez_compressed(save_path, glove=glove)\n",
        "        print(\"saved trimmed glove matrix at: {}\".format(save_path))\n",
        "\n",
        "\n",
        "def create_vocabulary(vocabulary_path, data_paths, tokenizer):\n",
        "    if not gfile.Exists(vocabulary_path):\n",
        "        print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, str(data_paths)))\n",
        "        vocab = {}\n",
        "        for path in data_paths:\n",
        "            with open(path, mode=\"rb\") as f:\n",
        "                counter = 0\n",
        "                for line in f:\n",
        "                    counter += 1\n",
        "                    if counter % 100000 == 0:\n",
        "                        print(\"processing line %d\" % counter)\n",
        "                    tokens = tokenizer(line)\n",
        "                    for w in tokens:\n",
        "                        if w in vocab:\n",
        "                            vocab[w] += 1\n",
        "                        else:\n",
        "                            vocab[w] = 1\n",
        "        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
        "        print(\"Vocabulary size: %d\" % len(vocab_list))\n",
        "        with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
        "            for w in vocab_list:\n",
        "                vocab_file.write(w + \"\\n\")\n",
        "\n",
        "\n",
        "def sentence_to_token_ids(sentence, vocabulary, tokenizer):\n",
        "  words = tokenizer(sentence)\n",
        "  return [vocabulary.get(w, UNK_ID) for w in words]\n",
        "\n",
        "\n",
        "def data_to_token_ids(data_path, target_path, vocabulary_path, tokenizer):\n",
        "    if not gfile.Exists(target_path):\n",
        "        print(\"Tokenizing data in %s\" % data_path)\n",
        "        vocab, _ = initialize_vocabulary(vocabulary_path)\n",
        "        with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
        "            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
        "                counter = 0\n",
        "                for line in data_file:\n",
        "                    counter += 1\n",
        "                    if counter % 5000 == 0:\n",
        "                        print(\"tokenizing line %d\" % counter)\n",
        "                    token_ids = sentence_to_token_ids(line, vocab, tokenizer)\n",
        "                    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # args = setup_args()\n",
        "    vocab_path = pjoin(\"/content/drive/My Drive/Deep Data/data/squad\", \"vocab.dat\")\n",
        "\n",
        "    train_path = pjoin(\"/content/drive/My Drive/Deep Data/data/squad/train\", \"train\")\n",
        "    valid_path = pjoin(\"/content/drive/My Drive/Deep Data/data/squad/val\", \"val\")\n",
        "    dev_path = pjoin(\"/content/drive/My Drive/Deep Data/data/squad/val\", \"dev\")\n",
        "\n",
        "    create_vocabulary(vocab_path,\n",
        "                      [pjoin(\"/content/drive/My Drive/Deep Data/data/squad/train\", \"train.context\"),\n",
        "                       pjoin(\"/content/drive/My Drive/Deep Data/data/squad/train\", \"train.question\"),\n",
        "                       pjoin(\"/content/drive/My Drive/Deep Data/data/squad/val\", \"val.context\"),\n",
        "                       pjoin(\"/content/drive/My Drive/Deep Data/data/squad/val\", \"val.question\")],\n",
        "                       basic_tokenizer)\n",
        "    vocab, rev_vocab = initialize_vocabulary(pjoin(\"/content/drive/My Drive/Deep Data/data/squad\", \"vocab.dat\"))\n",
        "\n",
        "\n",
        "\n",
        "    glove_dim = 100\n",
        "    source_dir = \"/content/drive/My Drive/Deep Data/data\"\n",
        "    glove_dir = \"/content/drive/My Drive/Deep Data/download/dwr\"\n",
        "    process_glove(glove_dir, glove_dim, rev_vocab, source_dir + \"/glove.trimmed.{}\".format(glove_dim),\n",
        "                  random_init=True)\n",
        "\n",
        "    # Creating Dataset \n",
        "    x_train_dis_path = train_path + \".ids.context\"\n",
        "    y_train_ids_path = train_path + \".ids.question\"\n",
        "    data_to_token_ids(train_path + \".context\", x_train_dis_path, vocab_path, basic_tokenizer)\n",
        "    data_to_token_ids(train_path + \".question\", y_train_ids_path, vocab_path, basic_tokenizer)\n",
        "\n",
        "    x_dis_path = valid_path + \".ids.context\"\n",
        "    y_ids_path = valid_path + \".ids.question\"\n",
        "    data_to_token_ids(valid_path + \".context\", x_dis_path, vocab_path, basic_tokenizer)\n",
        "    data_to_token_ids(valid_path + \".question\", y_ids_path, vocab_path, basic_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating vocabulary /content/drive/My Drive/Deep Data/data/squad/vocab.dat from data ['/content/drive/My Drive/Deep Data/data/squad/train/train.context', '/content/drive/My Drive/Deep Data/data/squad/train/train.question', '/content/drive/My Drive/Deep Data/data/squad/val/val.context', '/content/drive/My Drive/Deep Data/data/squad/val/val.question']\n",
            "Vocabulary size: 115373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 400000/400000.0 [31:46<00:00, 209.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "71733/115373 of word vocab have corresponding vectors in /content/drive/My Drive/Deep Data/download/dwr/glove.6B.100d.txt\n",
            "saved trimmed glove matrix at: /content/drive/My Drive/Deep Data/data/glove.trimmed.100\n",
            "Tokenizing data in /content/drive/My Drive/Deep Data/data/squad/train/train.context\n",
            "tokenizing line 5000\n",
            "tokenizing line 10000\n",
            "tokenizing line 15000\n",
            "tokenizing line 20000\n",
            "tokenizing line 25000\n",
            "tokenizing line 30000\n",
            "tokenizing line 35000\n",
            "tokenizing line 40000\n",
            "tokenizing line 45000\n",
            "tokenizing line 50000\n",
            "tokenizing line 55000\n",
            "tokenizing line 60000\n",
            "tokenizing line 65000\n",
            "tokenizing line 70000\n",
            "tokenizing line 75000\n",
            "Tokenizing data in /content/drive/My Drive/Deep Data/data/squad/train/train.question\n",
            "tokenizing line 5000\n",
            "tokenizing line 10000\n",
            "tokenizing line 15000\n",
            "tokenizing line 20000\n",
            "tokenizing line 25000\n",
            "tokenizing line 30000\n",
            "tokenizing line 35000\n",
            "tokenizing line 40000\n",
            "tokenizing line 45000\n",
            "tokenizing line 50000\n",
            "tokenizing line 55000\n",
            "tokenizing line 60000\n",
            "tokenizing line 65000\n",
            "tokenizing line 70000\n",
            "tokenizing line 75000\n",
            "Tokenizing data in /content/drive/My Drive/Deep Data/data/squad/val/val.context\n",
            "tokenizing line 5000\n",
            "Tokenizing data in /content/drive/My Drive/Deep Data/data/squad/val/val.question\n",
            "tokenizing line 5000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}